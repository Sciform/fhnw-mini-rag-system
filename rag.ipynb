{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd515023",
      "metadata": {
        "id": "dd515023"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sciform/fhnw-mini-rag-system/blob/main/rag2.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b41b939",
      "metadata": {
        "id": "0b41b939"
      },
      "outputs": [],
      "source": [
        "#%pip install --upgrade pip setuptools wheel\n",
        "%pip install langchain langchain-huggingface langchain-community faiss-cpu sentence-transformers transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25dcc9d",
      "metadata": {
        "id": "f25dcc9d"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "import os\n",
        "\n",
        "# Set your Hugging Face Hub API token (free signup)\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<Hugging_Face_Access>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2139c5",
      "metadata": {
        "id": "6b2139c5"
      },
      "outputs": [],
      "source": [
        "# 1. Load documents\n",
        "\n",
        "# Download the sample text files\n",
        "!wget https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample1.txt -P docs/\n",
        "!wget https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample2.txt -P docs/\n",
        "!wget https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample3.txt -P docs/\n",
        "\n",
        "\n",
        "loader1 = TextLoader(\"docs/sample1.txt\")\n",
        "loader2 = TextLoader(\"docs/sample2.txt\")\n",
        "loader3 = TextLoader(\"docs/sample3.txt\")\n",
        "documents = loader1.load() + loader2.load() + loader3.load()\n",
        "\n",
        "# 2. Split into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f54532f",
      "metadata": {
        "id": "1f54532f"
      },
      "outputs": [],
      "source": [
        "# 3. Embed and store in FAISS\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34e05d0",
      "metadata": {
        "id": "c34e05d0"
      },
      "outputs": [],
      "source": [
        "# Print the number of documents in the vector store (FAISS index)\n",
        "print(f\"Number of documents in vector store: {vector_store.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec990fb9",
      "metadata": {
        "id": "ec990fb9"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", k=1)\n",
        "query = \"What is a blue whale ?\"\n",
        "retrieved_docs = retriever.invoke(query, k=1)\n",
        "\n",
        "# Print the retrieved documents\n",
        "print(\"Documents retrieved:\")\n",
        "for doc in retrieved_docs[:len(retrieved_docs)]:\n",
        "    print(f\"Document: {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab2742d",
      "metadata": {
        "id": "0ab2742d"
      },
      "outputs": [],
      "source": [
        "# 4. Use a free LLM (small one)\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "model_falcon_base = \"tiiuae/falcon-rw-1b-instruct\"\n",
        "model_falcon_instruct = \"ericzzz/falcon-rw-1b-instruct-openorca\"\n",
        "model_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Wrap it in LangChain's new HuggingFaceEndpoint class\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=model_falcon_instruct,\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=75,\n",
        "    temperature=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe8fd56",
      "metadata": {
        "id": "bbe8fd56"
      },
      "outputs": [],
      "source": [
        "# 5. Create RetrievalQA chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer using only this context:\n",
        "    {context}\n",
        "\n",
        "    Question: {input}\"\"\"\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(\n",
        "    llm,\n",
        "    prompt,\n",
        "    document_separator=\"\\n\\n\")\n",
        "\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", k=1)\n",
        "\n",
        "qa_chain = create_retrieval_chain(\n",
        "    retriever,\n",
        "    document_chain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc3805e",
      "metadata": {
        "id": "1fc3805e"
      },
      "outputs": [],
      "source": [
        "# 6. Ask something\n",
        "query = \"Who climbs Mount Everest?\"\n",
        "context = \"Use only the most relevant document, if unsure say 'I don't know'. Answer as short as possible. Do not confuse animals.\"\n",
        "result = qa_chain.invoke({\"input\": query}, {\"context\": context})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f922dc",
      "metadata": {
        "id": "19f922dc"
      },
      "outputs": [],
      "source": [
        "print(\"\\nQuestion:\", query)\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e5e330",
      "metadata": {
        "id": "f5e5e330"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
